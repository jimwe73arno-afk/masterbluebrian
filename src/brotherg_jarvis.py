import streamlit as st
import google.generativeai as genai
import os
from dotenv import load_dotenv
from jarvis_memory import JarvisMemory
from jarvis_prompts import get_system_prompt

# é é¢è¨­å®š
st.set_page_config(page_title="BrotherG Jarvis", page_icon="ğŸ§ ", layout="wide")
load_dotenv()

# æ¨™é¡Œ
st.title("ğŸ§  BrotherG Jarvis - è¨ºæ–·æ¨¡å¼")

# API Key æª¢æŸ¥
api_key = os.getenv("GEMINI_API_KEY")
if not api_key and "GEMINI_API_KEY" in st.secrets:
    api_key = st.secrets["GEMINI_API_KEY"]

if not api_key:
    st.error("âŒ æ‰¾ä¸åˆ° GEMINI_API_KEYï¼Œè«‹æª¢æŸ¥ Secretsï¼")
    st.stop()
else:
    # éš±ç¢¼é¡¯ç¤º Key çš„å‰å¹¾ç¢¼ï¼Œç¢ºèªæœ‰è®€åˆ°
    masked_key = api_key[:5] + "..." + api_key[-3:]
    st.success(f"ğŸ”‘ API Key å·²è¼‰å…¥: {masked_key}")

genai.configure(api_key=api_key)

# åˆå§‹åŒ–è¨˜æ†¶
@st.cache_resource
def init_memory():
    return JarvisMemory()

try:
    memory = init_memory()
    st.success("ğŸ“š Firebase è¨˜æ†¶åº«é€£ç·šæˆåŠŸ")
except Exception as e:
    st.error(f"ğŸ”¥ Firebase é€£ç·šå¤±æ•—: {e}")
    st.stop()

# æ¨¡å‹é€£ç·šæ¸¬è©¦ (é¡¯ç¤ºè©³ç´°éŒ¯èª¤)
st.info("ğŸ”„ æ­£åœ¨å˜—è©¦é€£ç·š Gemini æ¨¡å‹...")

candidates = [
    "gemini-1.5-flash-latest",
    "gemini-1.5-flash",
    "gemini-2.0-flash-exp",
    "gemini-pro"
]

valid_model = None
error_logs = []

for model_name in candidates:
    try:
        model = genai.GenerativeModel(model_name)
        response = model.generate_content("Test")
        valid_model = model
        st.success(f"âœ… æˆåŠŸé€£ç·šæ¨¡å‹: **{model_name}**")
        break
    except Exception as e:
        error_msg = str(e)
        st.warning(f"âš ï¸ å˜—è©¦ {model_name} å¤±æ•—: {error_msg}")
        error_logs.append(f"{model_name}: {error_msg}")

if not valid_model:
    st.error("âŒ æ‰€æœ‰æ¨¡å‹é€£ç·šå¤±æ•—ã€‚è«‹æˆªåœ–æ­¤ç•«é¢å›å ±ã€‚")
    with st.expander("æŸ¥çœ‹è©³ç´°éŒ¯èª¤æ—¥èªŒ"):
        for log in error_logs:
            st.code(log)
    st.stop()

# --- å¦‚æœæˆåŠŸé€£ç·šï¼Œä¸‹é¢æ‰æ˜¯å°è©±ä»‹é¢ ---

if "messages" not in st.session_state:
    st.session_state["messages"] = []

for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("å•Ÿå‹•è¨ºæ–·å°è©±..."):
    st.chat_message("user").markdown(prompt)
    st.session_state["messages"].append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        try:
            # é€™è£¡ç°¡åŒ–æµç¨‹ï¼Œå°ˆæ³¨æ¸¬è©¦ç”Ÿæˆ
            system_prompt = get_system_prompt()
            response = valid_model.generate_content(f"{system_prompt}\n\nUser: {prompt}")
            answer = response.text
            
            message_placeholder.markdown(answer)
            st.session_state["messages"].append({"role": "assistant", "content": answer})
        except Exception as e:
            st.error(f"ç”Ÿæˆå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
